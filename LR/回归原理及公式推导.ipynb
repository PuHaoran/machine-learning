{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "问：如何从假设、损失函数、梯度来推导线性回归和逻辑回归？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归推导\n",
    "\n",
    "#### 1. 假设（使用最大似然估计解释最小二乘）\n",
    "\n",
    "假设误差是独立同分布的，服从均值为0，方差为$σ^{2}$的高斯分布。\n",
    "\n",
    "$y^{(i)} = \\theta^{T}*x^{(i)} + \\epsilon^{(i)}$\n",
    "\n",
    "\n",
    "#### 2. 损失函数\n",
    "\n",
    "高斯分布的概率密度函数\n",
    "\n",
    "$p(\\epsilon^{(i)}) = \\frac{1}{\\sqrt{2π}\\sigma}exp(-\\frac{(y^{(i)} - θ^{T}*x^{(i)})^{2}}{2\\sigma^{2}}$\n",
    "\n",
    "$p(y^{(i)}|x^{(i)};θ) = \\frac{1}{\\sqrt{2π}\\sigma}exp(-\\frac{(y^{(i)} - θ^{T}*x^{(i)})^{2}}{2\\sigma^{2}})$\n",
    "\n",
    "最大似然函数\n",
    "\n",
    "$L(θ)_{max} = \\Pi_{i=1}^m p(y^{i}|x^{(i)};θ) = \\Pi_{i=1}^m\\frac{1}{\\sqrt{2π}\\sigma}exp(-\\frac{(y^{(i)} - θ^{T}*x^{(i)})^{2}}{2\\sigma^{2}})$\n",
    "\n",
    "$l(θ)_{max}$ = $logL(θ)$ \n",
    "\n",
    "= $\\sum_{i=1}^m (log\\frac{1}{\\sqrt{2π}\\sigma} - \\frac{(y^{(i)} - θ^{T}*x^{(i)})^{2}}{2\\sigma^{2}})$\n",
    "\n",
    "= $m*log\\frac{1}{\\sqrt{2π}\\sigma} - \\sum_{i=1}^m\\frac{(y^{(i)} - θ^{T}*x^{(i)})^{2}}{2\\sigma^{2}})$\n",
    "\n",
    "=> $arg_{min}\\frac{1}{2}\\sum_{i=1}^m(y^{(i)} - θ^{T}*x^{(i)})^{2}$\n",
    "\n",
    "经过上述推导，我们得到最小二乘公式，如何由最小二乘求解θ值？\n",
    "\n",
    "假定X是m行n列的矩阵，从矩阵入手进行推导。\n",
    "m个n维样本组成矩阵X，则\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} 0 & 1 & 0  & ..\\\\ 0 & 1 & 0 & .. \\\\ 0 & 1 & 0 & .. \\\\ 0 & 1 & 0 & ..\\end{bmatrix}_{mxn}* \\space \\begin{bmatrix} θ_{0} \\\\ θ_{1} \\\\ . \\\\  .\\\\ . \\end{bmatrix}_{nx1}  = \\begin{bmatrix} 0 \\\\ 1 \\\\ . \\\\ . \\end{bmatrix}_{mx1}\n",
    "$$\n",
    "\n",
    "已知成本函数$J(θ) = \\frac{1}{2}(Xθ - Y)^{T} \\cdot (Xθ-Y)$，求θ取何值时，J(θ)取最小值？\n",
    "\n",
    "$J(θ) = \\frac{1}{2}(Xθ - Y)^{T} \\cdot (Xθ-Y) = \\frac{1}{2}(θ^{T}X^{T}Xθ - θ^{T}X^{T}Y - Y^{T}Xθ + YY^{T})$\n",
    "\n",
    "对J(θ)求导得，\n",
    "\n",
    "$\\frac{\\partial{J(θ)}}{\\partialθ} = \\frac{1}{2}(2X^{T}Xθ-X^{T}Y-X^{T}Y) = X^{T}Xθ-X^{T}Y$\n",
    "\n",
    "$X^{T}Xθ-X^{T}Y = 0$，得\n",
    "\n",
    "$θ=(X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "#### 梯度下降算法\n",
    "\n",
    "实际工作中，由于计算性能的影响，我们不可以直接采用矩阵运算的方式求解θ，这时对于成本函数J，我们要得到全局最小值，就需要用到梯度下降法了。（在高等数学中，当J中有一个参数，我们使用d代表求导，当J中有两个及以上参数时，则使用偏导数符号$\\partial$代表微分，这里没有严格区分均使用了$\\partial$符号）\n",
    "\n",
    "下面，我们求出成本函数J(θ)的导数，让参数θ不断迭代，使成本函数J沿着负梯度的方向下降，达到全局最小值。\n",
    "\n",
    "令$h_{θ}(x^{(i)}) = θ^{T} \\cdot x^{(i)}$，$J(θ) = \\frac{1}{2}\\sum_{i=1}^m(y^{(i)} - h_{θ}(x^{(i)}))^{2}$\n",
    "\n",
    "$\\frac{\\partial{J(θ)}}{\\partialθ_{j}} = \\sum_{i=1}^m(y^{(i)} - h_{θ}(x^{(i)})) \\cdot x_{j}^{(i)}$\n",
    "\n",
    "<img src=\"./imgs/img01.png\" width = \"350\" height = \"350\" />\n",
    "\n",
    "三种梯度下降算法，包括批量梯度下降（BGD）、随机梯度下降（SGD）、mini-batch SGD。一般我们所说的SGD指的是mini-batch SGD。\n",
    "\n",
    "1） BGD每更新一个参数会使用所有样本，这样每次参数迭代都会使用所有样本。当样本值m很大时，参数会消耗很多时间，可得到全局最优解。\n",
    "\n",
    "repeat { \n",
    "\n",
    "$$θ_{j}^{'} = θ_{j} + α\\frac{1}{m} \\sum_{i=1}^m(y^{(i)} - h_{θ}(x^{(i)})) \\cdot x_{j}^{(i)}$$\n",
    "\n",
    "}\n",
    "\n",
    "2） SGD通过每个样本来迭代一次，若样本量很大时，可能只用部分样本就已将θ迭代至最优了，SGD可能跳出局部最小值。\n",
    "\n",
    "repeat { \n",
    "\n",
    "    for i=1,...m{\n",
    "\n",
    "$$θ_{j}^{'} = θ_{j} + α\\frac{1}{m} \\sum_{i=1}^m(y^{(i)} - h_{θ}(x^{(i)})) \\cdot x_{j}^{(i)}$$\n",
    "    \n",
    "    }\n",
    "}\n",
    "\n",
    "3） 工程中，一般使用mini-batch SGD，不是拿到一个样本即更新梯度，也不是拿到所有样本才更新梯度，而是采用若干随机样本的平均梯度作为更新方向。\n",
    "\n",
    "假设总共1000个样本，每次更新参数取10个样本，则\n",
    "\n",
    "repeat { \n",
    "\n",
    "    for i=1, 11, 21,...991{\n",
    "\n",
    "$$θ_{j}^{'} = θ_{j} + α\\frac{1}{10} \\sum_{k=i}^{i+9}(y^{(k)} - h_{θ}(x^{(k)})) \\cdot x_{j}^{(k)}$$\n",
    "    \n",
    "    }\n",
    "}\n",
    "\n",
    "Min-batch的中间值如何选择？\n",
    "\n",
    "1. 样本值较小，直接选用batch grant descent。\n",
    "\n",
    "2. 样本值很大，一般mini-batch size: 64 128 256 512（2的次方电脑运行更快）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR\n",
    "\n",
    "#### 1.假设\n",
    "\n",
    "假设数据服从伯努利分布，通过极大似然函数的方法，运用梯度下降求解参数，来达到将数据二分的目的。\n",
    "\n",
    "#### 2. 损失函数\n",
    "\n",
    "$z = θ^{T}x，h_{θ}(x) = \\frac{1}{1+e^{-z}}$\n",
    "\n",
    "$p(y=1|x;θ) = h_{θ}(x), p(y=0|x;θ) = 1-h_{θ}(x)$\n",
    "\n",
    "其似然函数为\n",
    "\n",
    "$L(θ)_{max} = \\prod_{i=1}^{m}(h_{θ}(x^{(i)}))^{y^{(i)}}(1-h_{θ}(x^{(i)}))^{1-y^{(i)}}$\n",
    "\n",
    "对数似然函数为\n",
    "\n",
    "$l(θ) = \\prod_{i=1}^{m}(h_{θ}(x^{(i)}))^{y^{(i)}} \\cdot (1-h_{θ}(x^{(i)}))^{1-y^{(i)}}$ \n",
    "\n",
    "$= \\sum_{i=1}^{m}(y^{(i)} \\cdot logh_{θ}(x^{(i)})+(1-y^{(i)}) \\cdot log(1-h_{θ}(x^{(i)})))$\n",
    "\n",
    "最大对数似然函数加负号可转化为最小成本函数\n",
    "\n",
    "$J(θ) = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)} \\cdot logh_{θ}(x^{(i)})+(1-y^{(i)}) \\cdot log(1-h_{θ}(x^{(i)})))$\n",
    "\n",
    "#### 3. 梯度下降算法\n",
    "\n",
    "采用梯度下降算法沿着函数负梯度方向进行迭代。\n",
    "\n",
    "已知$h_{θ}(x^{(i)})$是sigmoid函数，则\n",
    "\n",
    "$h_{θ}^{'}(x^{(i)}) = h_{θ}(x^{(i)}) \\cdot (1-h_{θ}(x^{(i)})) \\cdot x_{j}$\n",
    "\n",
    "求导\n",
    "\n",
    "$\\frac{\\partial{J(θ)}}{\\partialθ_{j}} = -\\frac{1}{m}\\sum_{i=1}^{m}(\\frac{y^{(i)}}{h_{θ}(x^{(i)})} - \\frac{1-y^{(i)}}{1-h_{θ}(x^{(i)})}) \\cdot h_θ(x^{(i)}) \\cdot (1-h_{θ}(x^{(i)})) \\cdot x_{j}$\n",
    "\n",
    "$=-\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}-h_{θ}(x^{(i)})) \\cdot x_{j}$\n",
    "\n",
    "不难发现，线性回归和逻辑回归求导后得到的梯度下降式是一样的（PS：$h_{θ}(x^{(i)})$不相同，LR对线性回归用sigmoid函数做了非线性变换），它们都是广义线性模型。\n",
    "\n",
    "由于LR的三种梯度下降算法与线性回归的相同，这里不再赘述。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
