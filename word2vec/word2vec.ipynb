{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec\n",
    "\n",
    "- 详细介绍请参考：\n",
    "\n",
    "    - https://zhuanlan.zhihu.com/p/26306795\n",
    "    \n",
    "    - 《word2vec 中的数学原理详解》\n",
    "    \n",
    "    - https://www.cnblogs.com/iloveai/p/word2vec.html\n",
    "\n",
    "\n",
    "1. 为什么需要word2vec？\n",
    "\n",
    "one-hot编码维度很大，而且不能在语义层面上对词进行表达，所以我们考虑采用一个新的语言模型。\n",
    "\n",
    "2. 什么是word2vec？\n",
    "\n",
    "word2vec是一种神经网络语言模型，将词映射到一个低纬稠密的词嵌入向量，常用的是CBOW和Skip-gram模型。这个模型的输入是词的one-hot encoder，模型的输出是softmax，但其实最后的输出是什么我们并不关心，我们关心的是模型第一个隐含层中的参数权重，这个参数权重就是我们需要的词嵌入矩阵，根据词嵌入矩阵乘以某个词的one hot向量，我们就可以得到该词的词向量。\n",
    "\n",
    "3. 如何训练word2vec？\n",
    "\n",
    "    <img src=\"./imgs/img03.png\" width = \"450\" height = \"450\" />\n",
    "\n",
    "    这里说明一点，隐层的激活函数其实是线性的，所以word2vec可以理解为线性模型。这里我认为W和W'都可以作为我们的嵌入矩阵，一个每一行是一个嵌入向量，一个每一列是一个嵌入向量。\n",
    "\n",
    "    3.1 CBOW\n",
    "    \n",
    "    用一个词的上下文作为输入，来预测这个词本身，则这个模型叫做CBOW模型\n",
    "<img src=\"./imgs/img05.png\" width = \"250\" height = \"250\" />\n",
    "\n",
    "    3.2 skip-gram\n",
    "    \n",
    "    用一个词作为输入，来预测它周围的上下文，这个模型叫做skip-gram模型\n",
    "<img src=\"./imgs/img04.png\" width = \"250\" height = \"250\" />\n",
    "\n",
    "<img src=\"./imgs/img02.png\" width = \"300\" height = \"300\" />\n",
    "\n",
    "\n",
    "### 加速softmax的方法\n",
    "\n",
    "最后输出的是个维度很高的词概率向量，这样softmax分母进行加和操作会很慢，所以我们采取一些技巧加速这个求和过程。\n",
    "\n",
    "1. 层次softmax，使用分级的分类树\n",
    "\n",
    "本质是把 N 分类问题变成 log(N)次二分类\n",
    "\n",
    "为了避免计算所有词的softmax概率，word2vec使用霍夫曼树代替从隐藏层到softmax的映射。该方法不用为了获得最后的概率和而评估神经网络中的N个输出节点，而只需要评估大约log2(W)个节点。层次softmax使用一种二叉树结构来表示词典里的所有词，V个词都是二叉树的叶子节点，V-1个非叶子节点。\n",
    "\n",
    "2. 采用负采样\n",
    "\n",
    "本质是预测总体类别的一个子集\n",
    "\n",
    "原来的训练过程是，每次用一个训练样本，则更新所有的权重，这样其实十分耗时的，其实我们不需要更新全部的参数，只需要更新那部分和这个样本相关的参数即可。负采样的意思是每次训练样本最后输出的值其实只有一个词是1，其他不是上下文范围内的词都是0，那我们可以从是0的单词中采样一部分如5~20个词来进行参数更新，而不是使用全部的词。在模型中的提现是，原来的输入x是词，输出也是词；现在的输入x是(词，词)输出为0或1。原来是一个巨大的10000维度的softmax，计算成本很高，现在是10000个二分类，而且每次迭代我们只需要训练它们其中的几个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
