{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升（boosting）\n",
    "给定输入向量x和输出向量y组成的若干训练样本(x1, y1)，(x2, y2)...(xn, yn)，目标是找到近似函数F^(x)，使得损失函数L(y, F(x))的损失函数最小。\n",
    "\n",
    "$L(y, F(x))$的典型定义：\n",
    "    \n",
    "$L(y, F(x)) = \\frac{1}{2}(y-F(x))^2$ （假设y服从高斯分布）\n",
    "\n",
    "$L(y, F(x)) = |y-F(x)|$ （假设y服从拉普拉斯分布）\n",
    "\n",
    "假定最优函数为F* (x)，即：\n",
    "\n",
    "$F^*(x) = argminE_{(x, y)}[L(y, F(x))]$\n",
    "\n",
    "假定F(x)是一族基函数$f_{i}(X)$的加权和\n",
    "\n",
    "$F(x) = sum_{i=1}^M r_{i}f_{i}(x) + const $\n",
    "\n",
    "#### 提升算法推导\n",
    "梯度提升方法寻找最优解F(x)，使得损失函数在训练集上的期望最小。\n",
    "\n",
    "首先，给定常函数$F_{0}(x)$:\n",
    "$F_{0}(x) = argmin_{r} \\sum_{i=1}^n L(y_{i}, r)$\n",
    "\n",
    "以贪心的思路扩展得到$F_{m}(x)$\n",
    "\n",
    "$F_{m}(x) = F_{m-1}(x) + argmin_{f \\in{H}} \\sum_{i=1}^n L(y_{i}, F_{m-1}(x) + f(x))$\n",
    "\n",
    "这里给出一个形象化的例子：\n",
    "\n",
    "第一颗树给出预测值，第二棵树会对残差部分进行估计，估计残差再和残差继续做差，第三棵树同样对残差进行估计..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT（梯度提升决策树）\n",
    "\n",
    "梯度提升的典型基函数即决策树（尤其是CART）\n",
    "\n",
    "第m步的梯度提升是根据残差数据计算决策树$t_{m}(x)$。令树$t_{m}(x)$的叶节点数据为J，即树$t_{m}(x)$将输入空间划分为J，即树$t_{m}(x)$将输入空间划分为J个不相交区域$R_{1m}, R_{2m},...,R_{Jm}$，并且决策树$t_{m}(x)$可以在每个区域中给出某个类型的确定性预测。使用指示记号I(x)，对于输入x，$t_{m}(x)$为：\n",
    "<img src=\"./imgs/img06.png\" width = \"200\" height = \"200\" />\n",
    "\n",
    "使用线性搜索计算学习率，最小化损失函数\n",
    "\n",
    "$F_{m}(x)=F_{m-1}(x) + \\gamma \\cdot t_{m}(x_{i})$\n",
    "\n",
    "$r_{m}=argmin_{r}\\sum_{i=1}^{n}L(y_{i}, F_{m-1}(x_{i}) + \\gamma \\cdot t_{m}(x_{i}))$\n",
    "\n",
    "进一步，对树的每个区域分别计算步长，从而系数$b_{jm}$被合并到步长中，从而：\n",
    "\n",
    "$F_{m}(x) = F_{m-1}(x)+\\sum_{j=1}^{J}\\gamma{jm}I(x\\in R_{jm})$\n",
    "\n",
    "$\\gamma_{jm}=argmin_{r}\\sum_{x_{i}\\in R_{jm}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma \\cdot t_{m}(x_{i}))$\n",
    "\n",
    "（这里要强调一个容易混淆的概念，L是残差的损失函数，$F_{m-1}(x_{i})$是上一棵树的预测值$argmin_{r}\\sum_{x_{i}\\in R_{jm}}L(y_{i}, F_{m-1}(x_{i}) + \\gamma \\cdot t_{m}(x_{i}))$是目标函数）\n",
    "\n",
    "GBDT并非对整个树做权值的调整，而是对每一个叶子做权值的调整。\n",
    "\n",
    "#### 参数设置和正则化\n",
    "\n",
    "- 1.对训练集拟合过高会降低模型的泛化能力，需要使用正则化来降低过拟合\n",
    "    - 1.1 对复杂模型增加惩罚项，如：模型复杂度正比于叶节点数目或叶节点预测值的平方和\n",
    "    - 1.2 对决策树剪枝\n",
    "- 2.叶节点数目控制了树的层数，一般选择4<=J<=8\n",
    "- 3. 叶节点包含的最小样本数目\n",
    "    - 3.1 防止出现过小的叶节点，降低预测的方差\n",
    "- 4.梯度提升迭代次数M\n",
    "    - 4.1 增加M可降低训练集的损失值，但有过拟合风险\n",
    "    - 4.2 交叉验证\n",
    "    \n",
    "#### GBDT总结\n",
    "- 1.损失函数时最小平方误差、绝对值误差等，则为回归问题；而损失函数为交叉熵或KL-散度，则为分类问题。\n",
    "\n",
    "- 2.对目标函数分解成若干基函数的加权和，是常见的技术手段：神经网络、径向基函数、傅里叶小波变换、SVM等都可以看到它的影子。\n",
    "\n",
    "- 3.只使用一阶导数信息。\n",
    "\n",
    "- 4.GBDT每一轮关注真实值和预测值的残差，然后下一轮以本轮的残差为输入，尽量去拟合这个残差，使得每一轮的残差不断减小。所以GBDT一定向损失函数减小的方向变化，而传统的boosting算法只能尽可能的向梯度方向减小。这是GBDT相对于传统boosting的算法的最大区别，这也是为什么GBDT相比传统Boosting算法可以用更少的树个数与深度达到更好的效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost核心推导过程\n",
    "\n",
    "#### 考虑使用二阶导信息\n",
    "求第t颗决策树的目标函数，$J(f)$是Xgboost的损失函数。\n",
    "\n",
    "$f_{t}(x_{i})$就是第t棵树对样本$x_{i}$的预测值，预测值本身是残差部分。\n",
    "\n",
    "<img src=\"./imgs/img01.png\" width = \"350\" height = \"350\" />\n",
    "\n",
    "其中目标函数的解释，\n",
    "<img src=\"./imgs/img02.jpg\" width = \"350\" height = \"350\" />\n",
    "\n",
    "最开始一棵树，然后如果是MAE选用中位数预测y，如果是MSE选用平均数预测y，然后第二棵树就在这个基础上再次求导。\n",
    "\n",
    "#### 正则项的定义\n",
    "决策树的复杂度可考虑叶结点数和叶权值，如使用叶结点总数和叶权值平方和的加权。\n",
    "这里$w_{j}$为叶子节点的预测值\n",
    "$\\Omega(f_{t}) = \\Upsilon T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_{j}^{2}$\n",
    "\n",
    "#### 目标函数计算\n",
    "<img src=\"./imgs/img03.png\" width = \"300\" height = \"300\" />\n",
    "\n",
    "n--------样本个数\n",
    "\n",
    "$y_{i}$--------第i个样本的真实值\n",
    "\n",
    "$y_{i}^{^{t-1}}$--------第t-1个决策树的第i个样本的预测值\n",
    "\n",
    "T--------当前叶子节点个数\n",
    "\n",
    "$\\sum_{i\\in I_{j}}g_{i}$--------位于j号叶子的所有样本的梯度的和\n",
    "\n",
    "$f_{t}(x_{i})$--------第i个样本的预测值\n",
    "\n",
    "$w_{q}(x_{i})$--------第i个样本在哪个叶子（与上面的等价）\n",
    "\n",
    "#### 目标函数继续化简\n",
    "<img src=\"./imgs/img04.png\" width = \"300\" height = \"300\" />\n",
    "\n",
    "#### 构造树的结构\n",
    "问：对于当前节点，如何进行子树划分？\n",
    "<img src=\"./imgs/img05.png\" width = \"300\" height = \"300\" />\n",
    "\n",
    "借鉴ID3/C4.5/CART的做法，使用贪心法：\n",
    "    \n",
    "    1）对于某可行划分，计算划分后的J(f)\n",
    "    2)对于所有可行划分，选择J(f)降低到最小的分割点\n",
    "\n",
    "以上就是XGBoost的核心推导过程。\n",
    "相对于传统的GBDT，XGBoost使用了二阶信息，可以更快地在训练集上收敛。\n",
    "\n",
    "#### 总结\n",
    "xgboost采用第一棵树取均值，第二颗树真实值和第二颗树的预测值的差值重新构建树，树的代价函数根据Taylor展式进行转换，新的代价函数包含一阶导和二阶导信息。令损失函数相对于$w_{j}$的导数为0可以求出最小化损失函数各个叶子节点的预测值。通过将预测值带入损失函数可求得损失函数的最小值，容易计算节点分裂前后损失函数的差值。Xgboost就是采用最大化这个差值为准则来进行决策树的构建。\n",
    "\n",
    "#### Xgboost与GBDT的区别\n",
    "\n",
    "1. GBDT算法基于成本函数的负梯度来构造新的决策树，只有在决策树完成构建后才进行剪枝，而XGBoost在决策树构建阶段就加入了正则项。\n",
    "\n",
    "2. GBDT在模型训练时只使用了代价函数的一阶导信息，而XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。\n",
    "\n",
    "3. 传统的GBDT使用CART作为基分类器，而XGBoost支持多种类型的基分类器，比如线性分类器。\n",
    "\n",
    "4. 传统的GBDT在每轮迭代使用全部数据，而XGBoost使用随机森林相似的策略，支持对数据的采样。\n",
    "\n",
    "5. 传统的GBDT没有设计对缺失值的处理，XGBoost能够学习出缺失值处理策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林计算特征重要度\n",
    "随机森林是常用的衡量特征重要性的方法。\n",
    "计算正例经过的节点，使用经过节点的样本数目、经过节点的gini系数和等指标。或者随机替换一列数据，重新建立决策树，计算模型的正确率的变换，从而考虑这一列的特征重要度。\n",
    "特征重要度的判断依据可以是：\n",
    "\n",
    "    1）经过节点的样本数目\n",
    "    2）经过节点的gini系数和\n",
    "    3）随机替换一列特征，重新构树，计算模型正确率变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 孤立森林（Isolation Forest）\n",
    "从根节点要叶子节点的路径长度，可以度量样本是不是噪声的一个指标。\n",
    "随机选择特征、随机选择分割点，生成一定深度的决策树，若干颗iTree组成iForest\n",
    "    \n",
    "    1）计算iTree中样本x从跟到叶子节点的长度f(x)\n",
    "    2）计算iForest中f(x)的总和F(x)\n",
    "若样本x是异常值，它应该在大多数的iTree中很快到达叶子，即F(x)较小。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
