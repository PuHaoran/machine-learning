{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林计算特征重要度\n",
    "随机森林是常用的衡量特征重要性的方法。\n",
    "计算正例经过的节点，使用经过节点的样本数目、经过节点的gini系数和等指标。或者随机替换一列数据，重新建立决策树，计算模型的正确率的变换，从而考虑这一列的特征重要度。\n",
    "特征重要度的判断依据可以是：\n",
    "\n",
    "    1）经过节点的样本数目\n",
    "    2）经过节点的gini系数和\n",
    "    3）随机替换一列特征，重新构树，计算模型正确率变化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 孤立森林（Isolation Forest）\n",
    "从根节点要叶子节点的路径长度，可以度量样本是不是噪声的一个指标。\n",
    "随机选择特征、随机选择分割点，生成一定深度的决策树，若干颗iTree组成iForest\n",
    "    \n",
    "    1）计算iTree中样本x从跟到叶子节点的长度f(x)\n",
    "    2）计算iForest中f(x)的总和F(x)\n",
    "若样本x是异常值，它应该在大多数的iTree中很快到达叶子，即F(x)较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 提升（boosting）\n",
    "给定输入向量x和输出向量y组成的若干训练样本(x1, y1)，(x2, y2)...(xn, yn)，目标是找到近似函数F^(x)，使得损失函数L(y, F(x))的损失函数最小。\n",
    "\n",
    "$L(y, F(x))$的典型定义：\n",
    "    \n",
    "$L(y, F(x)) = \\frac{1}{2}(y-F(x))^2$ （假设y服从高斯分布）\n",
    "\n",
    "$L(y, F(x)) = |y-F(x)|$ （假设y服从拉普拉斯分布）\n",
    "\n",
    "假定最优函数为F* (x)，即：\n",
    "\n",
    "$F^*(x) = argminE_{(x, y)}[L(y, F(x))]$\n",
    "\n",
    "假定F(x)是一族基函数$f_{i}(X)$的加权和\n",
    "\n",
    "$F(x) = sum_{i=1}^M r_{i}f_{i}(x) + const $\n",
    "\n",
    "### 提升算法框架\n",
    "梯度提升方法寻找最优解F(x)，使得损失函数在训练集上的期望最小。\n",
    "\n",
    "首先，给定常函数$F_{0}(x)$:\n",
    "$F_{0}(x) = argmin_{r} \\sum_{i=1}^n L(y_{i}, r)$\n",
    "\n",
    "以贪心的思路扩展得到$F_{m}(x)$\n",
    "\n",
    "$F_{m}(x) = F_{m-1}(x) + argmin_{f \\in{H}} \\sum_{i=1}^n L(y_{i}, F_{m-1}(x) + f(x))$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xgboost核心推导过程\n",
    "\n",
    "### 考虑使用二阶导信息\n",
    "求第t颗决策树的目标函数，$J(f)$是GBDT或Xgboost的损失函数。\n",
    "\n",
    "$f_{t}(x_{i})$就是第t棵树对样本$x_{i}$的预测值，预测值本身是残差部分。\n",
    "\n",
    "![image.png](./imgs/img01.png)\n",
    "其中目标函数的解释，\n",
    "![image.png](./imgs/img02.jpg)\n",
    "\n",
    "最开始一棵树，然后如果是MAE选用中位数预测y，如果是MSE选用平均数预测y，然后第二棵树就在这个基础上再次求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正则项的定义\n",
    "决策树的复杂度可考虑叶结点数和叶权值，如使用叶结点总数和叶权值平方和的加权。\n",
    "这里$w_{j}$为叶子节点的预测值\n",
    "$\\Omega(f_{t}) = \\Upsilon T + \\frac{1}{2} \\lambda \\sum_{j=1}^T w_{j}^{2}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标函数计算\n",
    "![image.png](./imgs/img03.png)\n",
    "\n",
    "n--------样本个数\n",
    "\n",
    "$y_{i}$--------第i个样本的真实值\n",
    "\n",
    "$y_{i}^{^{t-1}}$--------第t-1个决策树的第i个样本的预测值\n",
    "\n",
    "T--------当前叶子节点个数\n",
    "\n",
    "$\\sum_{i\\in I_{j}}g_{i}$--------位于j号叶子的所有样本的梯度的和\n",
    "\n",
    "$f_{t}(x_{i})$--------第i个样本的预测值\n",
    "\n",
    "$w_{q}(x_{i})$--------第i个样本在哪个叶子（与上面的等价）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标函数继续化简\n",
    "![image.png](./imgs/img04.png)\n",
    "\n",
    "### 构造树的结构\n",
    "问：对于当前节点，如何进行子树划分？\n",
    "![image.png](./imgs/img05.png)\n",
    "借鉴ID3/C4.5/CART的做法，使用贪心法：\n",
    "    \n",
    "    1）对于某可行划分，计算划分后的J(f)\n",
    "    2)对于所有可行划分，选择J(f)降低到最小的分割点\n",
    "\n",
    "以上就是XGBoost的核心推导过程。\n",
    "相对于传统的GBDT，XGBoost使用了二阶信息，可以更快地在训练集上收敛。\n",
    "\n",
    "### 总结\n",
    "gboost采用第一棵树取均值，第二颗树真实值和第二颗树的预测值的差值重新构建树，树的损失函数根据Taylor展式进行转换，新的目标函数包含一阶导和二阶导信息。令损失函数相对于$w_{j}$的导数为0可以求出最小化损失函数各个叶子节点的预测值。通过将预测值带入损失函数可求得损失函数的最小值，容易计算节点分裂前后损失函数的差值。Xgboost就是采用最大化这个差值为准则来进行决策树的构建。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
