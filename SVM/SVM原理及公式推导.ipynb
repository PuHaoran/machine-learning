{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "支持向量机的学习策略是间隔最大化，可形式化为求解凸二次规划的问题（即约束型最小二乘的求解），也等价于正则化的合页损失函数的最小化问题，下面我们通过一步步推导，来了解SVM。\n",
    "\n",
    "### 最大间隔\n",
    "\n",
    "用一对平行的边界将正负样本分隔开，其中线就是超平面。\n",
    "<img src=\"./imgs/img01.jpg\" width = \"200\" height = \"200\" />\n",
    "\n",
    "假设中线的法向量是w，选定边界上的任一正样本为u，由图知$w \\cdot u > C$（C为原点到超平面的距离），若$C = -b$，则$w \\cdot u + b > 0$；\n",
    "\n",
    "若边界到超平面的距离为1，则对于所有正样本有$w \\cdot u+b>=1$；对于所有负样本$w \\cdot u+b<=-1$，连接边界上的两个正负样本，做差可得到一个向量，将且点乘w的单位向量，即可得到两个边界距离。\n",
    "\n",
    "令正负样本的标签分别为$y=1$和$y=-1$，则无论正负样本都有$y(w \\cdot x+b)-1>=0$\n",
    "\n",
    "白板过程如下\n",
    "<img src=\"./imgs/img02.png\" width = \"200\" height = \"200\" />\n",
    "最大化margin，相当于$min\\parallel{w} \\parallel$，相当于$min\\frac{1}{2}\\left \\|{w}\\right \\|^{2}$\n",
    "\n",
    "于是得到下面线性可分支持向量机的最优化问题\n",
    "\n",
    "$min_{w,b}\\frac{1}{2}\\left \\|{w}\\right \\|^{2}$，\n",
    "\n",
    "$s.t.$ $y_{i}(w^{T} \\cdot x_{i}+b)-1\\geq 0, i=1,2...N$\n",
    "\n",
    "这个带约束求极值问题是支持向量机的基本型。\n",
    "\n",
    "#### 什么是拉格朗日乘子法？\n",
    "\n",
    "基本的拉格朗日乘子法就是求函数f(x1,x2,...)在约束条件g(x1,x2,...)=0下的极值的方法。\n",
    "\n",
    "其主要思想是将约束条件函数与原函数联立，从而求出使原函数取得极值的各个变量的解。\n",
    "\n",
    "1.需要求极值的目标函数为\n",
    "\n",
    "$minf(x)$，且$g(x) \\le 0$\n",
    "\n",
    "2.定义一个新函数\n",
    "\n",
    "$λ \\ge 0$，$L(x, λ) = f(x) + λ \\cdot g(x)$\n",
    "\n",
    "3.用偏导数方法列方程\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{x}}=0$\n",
    "\n",
    "$\\frac{\\partial{L}}{\\partial{λ}}=0$\n",
    "\n",
    "4.求出x、λ的值，带入目标函数即可求出极值。\n",
    "\n",
    "#### 对偶算法\n",
    "为了求解上述的线性可分最优化问题，将它作为原始最优化问题，应用拉格朗日对偶性，通过求解对偶问题得到原始问题的最优解。这是线性可分支持向量机的对偶算法。那为何要这样做呢？\n",
    "    \n",
    "    1.对偶问题更容易求解\n",
    "    2.自然引入核函数，进而推广到非线性问题\n",
    "   \n",
    "$min_{w,b}\\frac{1}{2}\\left \\|{w}\\right \\|^{2}$，$s.t.$ $y_{i}(w^{T} \\cdot \\Phi(x_{i})+b)\\geq 1, i=1,2...N$\n",
    "\n",
    "应用拉格朗日乘子法得\n",
    "\n",
    "$L(w,b,α)=\\frac{1}{2}\\left \\|{w}\\right \\|^{2}-\\sum_{i=1}^n α_{i}(y_{i}(w^{T}\\cdot \\Phi(x_{i})+b)-1)$\n",
    "\n",
    "原问题是极小极大问题\n",
    "\n",
    "$min_{w,b}max_{α}L(w,b,α)$\n",
    "\n",
    "原始问题的对偶问题，是极大极小问题\n",
    "\n",
    "$max_{α}min_{w,b}L(w,b,α)$\n",
    "\n",
    "对偶问题具体如何由极小极大问题转化为极大极小问题，后面会在介绍拉格朗日部分给出更为详细的解释。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L=\\frac{1}{2}\\left \\|{w}\\right \\|^{2}-\\sum_{i=1}^n α_{i}(y_{i}(w\\cdot x_{i}+b)-1)-----①$\n",
    "\n",
    "令$\\frac{\\partial L}{\\partial w} = w-\\sum_{i=1}^n α_{i}y_{i}x_{i}=0$\n",
    "\n",
    "=> $w = \\sum_{i=1}^n α_{i}y_{i}x_{i}-----②$\n",
    "\n",
    "令$\\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^{n}α_{i}y_{i}$\n",
    "\n",
    "$=>\\sum_{i=1}^{n}α_{i}y_{i}=0-----③$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将②③带入①得，目标函数为\n",
    "\n",
    "$L=max_{α}\\sum_{i=1}^n α_{i}-\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^nα_{i}α_{j}y_{i}y_{j}x_{i}x_{j}$\n",
    "\n",
    "$s.t. \\sum_{i=1}^{n}α_{i}y_{i}=0$\n",
    "\n",
    "$α_{i}\\ge 0, i=1,2,3,...n$\n",
    "\n",
    "添加负号，目标函数转化为\n",
    "\n",
    "$L=min_{α}\\frac{1}{2}\\sum_{i=1}^n\\sum_{j=1}^nα_{i}α_{j}y_{i}y_{j}x_{i}x_{j}-\\sum_{i=1}^n α_{i}$\n",
    "\n",
    "$s.t. \\sum_{i=1}^{n}α_{i}y_{i}=0$\n",
    "\n",
    "$α_{i}\\ge 0, i=1,2,3,...n$\n",
    "\n",
    "此时，可求得最优解$α^{*}$(在样本很少很少的情况下，假设已知x、y，此时可通过求驻点的方式，得到α值)\n",
    "\n",
    "已知，所有样本满足$y=wx+b$，则此时可求出w、b\n",
    "\n",
    "$w^{*}=\\sum_{i=1}^{n}α_{i}y_{i}x_{i}$\n",
    "\n",
    "$b^{*}=y_{j}-\\sum_{i=1}^{n}α_{i}y_{i}(x_{i}x_{j})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求得分离超平面\n",
    "\n",
    "$w^{*}x + b^{*}=0$\n",
    "\n",
    "分类决策函数\n",
    "\n",
    "$f(x)=sign(w^{*}x + b^{*})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 软间隔最大化\n",
    "\n",
    "若数据线性不可分，则增加松弛因子$\\xi \\ge 0$，使函数间隔加上松弛变量大于等于1，这样约束条件变成\n",
    "\n",
    "$y_{i}(w\\cdot x_{i}+b)\\ge 1-\\xi\\$\n",
    "\n",
    "目标函数：\n",
    "$min_{w,b}\\frac{1}{2}\\parallel{w} \\parallel^{2}+C\\sum_{i=1}^{N}\\xi_{i}$\n",
    "\n",
    "这里C>0，称为惩罚参数，C越大对误分类的惩罚增加，C减小对误分类的惩罚减小。这意味着，C越大，模型泛化能力降低，过度带宽度越窄。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数\n",
    "<img src=\"./imgs/img05.png\" width = \"200\" height = \"200\" />\n",
    "SVM的损失函数为hige loss，因为类似一个合着的书页，所以又叫合页损失\n",
    "\n",
    "$L(y(w\\cdot x+b))=[1-y(w\\cdot x+b]_{+}$，下标+表示\n",
    "\n",
    "$[z]_{+}\\left\\{\n",
    "             \\begin{array}{lr}\n",
    "             z, z>0\\\\\n",
    "             0, z\\le 0 &  \n",
    "             \\end{array}\n",
    "\\right.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
