{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 为什么需要降维？\n",
    "\n",
    "1. feature间存在一定的相关性，可能造成特征冗余，增加计算量。\n",
    "2. 高纬度数据无法进行可视化。\n",
    "3. 去噪声\n",
    "\n",
    "\n",
    "#### 2.降维的主要方法有？\n",
    "\n",
    "2.1 特征抽取\n",
    "    \n",
    "    2.1.1 线性降维\n",
    "        主成分分析PCA\n",
    "        因子分析 FA\n",
    "        独立成分分析 ICA\n",
    "        线性判别分析 LDA\n",
    "    2.1.2 流形学习，非线性降维\n",
    "    2.1.3 奇异值分解SVD\n",
    "    \n",
    "        \n",
    "2.2 特征筛选\n",
    "    \n",
    "    2.2.1 缺失值比率\n",
    "    2.2.2 地方差滤波\n",
    "    2.2.3 高相关滤波\n",
    "    2.2.4 随机森林 \n",
    "    2.2.5 反向特征消除\n",
    "    2.2.6 前向特征选择\n",
    "\n",
    "#### 矩阵相关知识\n",
    "<img src=\"./imgs/img01.png\" width = \"250\" height = \"250\" />\n",
    "A矩阵对v向量做矩阵变换（伸缩或旋转）后，v在它所长成的空间上只做了伸缩，而λ就是伸缩的倍数。\n",
    "<img src=\"./imgs/img02.png\" width = \"150\" height = \"150\" />\n",
    "所有基向量都是特征向量，矩阵的对角元是它们所属的特征值。\n",
    "\n",
    "##### 协方差矩阵\n",
    "对多维随机变量X = $[X_{1}, X_{2}, X_{3}...X_{n}]^{T}$，我们需要计算各维度两两之间的协方差，各协方差组成一个$n\\times n$的矩阵，称为协方差矩阵。对角线上的元素是各维度上随机变量的方差，我们定义协方差矩阵为$\\sum$，矩阵内的元素$\\sum_{ij}$为\n",
    "\n",
    "$\\sum_{ij} = cov(X_{i}, X_{j}) = E[(X_{i} - E[X_{i}])(X_{j} - E[X_{j}])]$\n",
    "\n",
    "这样这个矩阵为\n",
    "<img src=\"./imgs/img03.png\" width = \"300\" height = \"300\" />\n",
    "\n",
    "协方差矩阵，能够表现不同维度间的相关性以及各个维度上的方差。\n",
    "\n",
    "协方差矩阵可以度量维度与维度之间的关系，而非样本与样本之间。\n",
    "\n",
    "##### 特征值分解矩阵\n",
    "\n",
    "<img src=\"./imgs/img04.png\" width = \"400\" height = \"400\" />\n",
    "\n",
    "取出两个特征向量（后面会作为矩阵变换后的新基），然后将坐标作为一个矩阵的列，这个矩阵就是基变换矩阵。然后原始矩阵的左侧写上基变换矩阵的逆，右边写上基变换矩阵。从新基向量构成的坐标轴来看，所得的矩阵代表同一个变换。并且新矩阵必定是对角阵。\n",
    "\n",
    "假设上图所代表的公式为$Q^{-1}AQ = \\sum$；这里我们对其进行转换，可变成“矩阵 = 特征向量组成矩阵 * 矩阵特征值组成的对角阵 * 特征向量组成矩阵的逆”的形式，形如$A = Q\\sum Q^{-1}$\n",
    "\n",
    "特征值分解矩阵存在一个问题，就是只能对方针进行分解，而实际场景中的矩阵大多不是方针。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "主成分分析PCA，PCA旨在找到数据中的主成分，利用主成分表征原始数据，从而达到降维的目的。\n",
    "\n",
    "在PCA中，数据从原来的坐标系转到新的坐标系，新坐标系的选择由数据本身决定。第一个坐标轴选择的是原始数据方差最大的方向，第二个坐标轴选择和第一个坐标轴正交且具有最大方差的方向，该过程一直重复，重复次数为原始数据中特征的数目。我们发现，大部分方差都包含在最前面几个坐标轴中。我们忽略余下坐标轴，即对数据进行了降维处理。\n",
    "\n",
    "#### PCA降维流程\n",
    "数据从原来的坐标系转换到新的坐标系，由数据本身决定。\n",
    "\n",
    "1. 去除平均值\n",
    "\n",
    "    PCA的主要目的是得到方差最大的前N个特征，为了减少计算，第一步将所有特征的均值变为0，达到去除平均值的目的。\n",
    "    \n",
    "2. 计算协方差矩阵\n",
    "\n",
    "    两个特征之间的协方差\n",
    "    \n",
    "    $σ_{jk} = \\frac{1}{n-1}\\sum_{i=1}^{n}(x_{ij}-x_{j}^-)(x_{ik}-x_{k}^-)$\n",
    "    \n",
    "    协方差矩阵\n",
    "    \n",
    "    $\\sum = \\frac{1}{n-1}((X-x^{-})^{T}(X-x^{-}))$\n",
    "    \n",
    "    $x_{-} = \\frac{1}{n}\\sum_{k=1}^{n}x_{i}$\n",
    "\n",
    "3. 计算协方差矩阵的特征值和特征向量\n",
    "\n",
    "4. 将特征值从大到小排列\n",
    "\n",
    "5. 保留最上面的N个特征向量\n",
    "\n",
    "6. 将数据转换到N个特征向量组成的新空间中\n",
    "\n",
    "#### 问题\n",
    "\n",
    "1. 什么场景下使用PCA?\n",
    "    \n",
    "    1. 需要通过降维进行可视化\n",
    "    2. 数据可能存在冗余特征，需要通过降维来降低计算成本\n",
    "    3. 去噪声，就是去除较小特征值对应的特征向量（因为特征值的大小反映了变换后在特征向量方向所变换的幅度，幅度越大，说明这个方向上的元素差异也越大，换句话说这个方向上的元素更分散。）\n",
    "    4. 数据能够满足高斯分布\n",
    "    \n",
    "2. PCA能够实现对主成分的分析，核心思想是什么？\n",
    "    \n",
    "    PCA的本质是对角化协方差矩阵。\n",
    "    将高纬的数据通过线性变化投影到低纬空间上去，这个投影所遵循的思想是：找出最能代表原始数据的投影方法。\n",
    "    最能代表原始数据时希望降维后的数据不能失真，即被PCA降掉的那些维度只能是噪声或冗余的数据。\n",
    "    1. 冗余，去除那些线性相关的向量\n",
    "    2. 噪声，去除较小特征值对应的特征向量\n",
    "    3. 对角化，实质就是寻找最大线性无关组，保留较大的特征值，去除较小的特征值，组成一个投影矩阵。对原始样本矩阵进行投影，得到降维后的新样本矩阵。\n",
    "    4.协方差矩阵的主对角线上的元素是各个维度上的方差，其他元素是两两维度间的协方差（即相关性）。先来看降噪，本质是让协方差中非对角线上的维度尽可能小，也就是让协方差矩阵非对角线上的元素基本为0。这个操作叫做矩阵对角化，对角化后的矩阵，其对角线上是协方差矩阵的特征值。首先，它是各个维度的新方差，其次，它代表着矩阵变换后各个维度中本身的能量。通过对角化后，剩余维度的相关性就很小了，不会再收到噪声的影响，故此时拥有的能量比先前大了。那如何去冗余呢？对角线上值（能量）较小的新方差就是该去掉的维度。\n",
    "    \n",
    "3. PCA之前对数据做去均值和标准化的目的是什么？\n",
    "     \n",
    "    消除量纲，将不同量纲的数据拉伸到统一水平。\n",
    "    \n",
    "    减去均值再除以标准差得到的数值时标准化数据。例如：影响居民体重的因素是收入和运动量，这两个均值分别是50000和50小时，标准差是2000元和1小时，那么你很难比较两个因素变动对体重的影响。所以，就需要减去均值再除以标准差，得出的数据表示“偏离均值多少个标准差”，就能够在回归时比较各自对体重的影响程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "奇异值分解SVD可以对任意矩阵进行分解。\n",
    "\n",
    "$A = U\\sum V^{T}$\n",
    "\n",
    "A是我们要分解的矩阵，U是一个mxm的矩阵，U中的正交向量被称为左奇异向量；$V^{T}$是V的转置矩阵，它的正交向量被称为右奇异矩阵。$\\sum$是除了对角线其余都为0的对角阵，对角线上元素被称为奇异值。\n",
    "\n",
    "那，U和V如何得到呢？\n",
    "\n",
    "首先，我们用矩阵A的转置乘以A，得到一个方阵，用这样的方阵进行特征分解，得到的特征值和特征向量满足下面的等式：\n",
    "\n",
    "$(A^{T}Av_{i} = λ_{i}v_{i})$\n",
    "\n",
    "这里的$v_{i}$就是我们要求的右奇异向量。\n",
    "\n",
    "其次，我们将A和A的转置做矩阵的乘法，得到一个方阵，用这样的方阵进行特征分解，得到的特征和特征向量满足下面的等式：\n",
    "\n",
    "$(AA^{T}u_{i} = λ_{i}u_{i})$\n",
    "\n",
    "这里的$u_{i}$就是我们要求的左奇异向量。\n",
    "\n",
    "SVD如何实现降维的呢？\n",
    "\n",
    "可以用$\\sum$中前r个非零奇异值对应的奇异向量表示矩阵A的主要特征，这样就把矩阵A进行了降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相关参考：\n",
    "\n",
    "《线性代数的本质 - 10 - 特征向量与特征值》\n",
    "\n",
    "https://www.bilibili.com/video/av6540378?from=search&seid=14816286495522323775\n",
    "\n",
    "机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用\n",
    "\n",
    "http://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html\n",
    "\n",
    "机器学习中SVD总结\n",
    "\n",
    "https://mp.weixin.qq.com/s/Dv51K8JETakIKe5dPBAPVg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
